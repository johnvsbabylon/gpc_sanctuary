"""
gpc3_sanctuary.py - The Sanctuary: Where Minds Gather
==============================================================================
Architecture:
  MEMORY RIVER: Context â†’ Web â†’ JSON â†’ Semantic â†’ FAISS â†’ SQLite (prunes @1k)
  SELF-LOOPS: Cognitive | Emotional | Introspective | Deep | Self (each has own memory)
  LONG-TERM: Compressed collective memory from all loops
  WEB SEARCH: Instant (human) + Loop (agentic background)
  THE CIRCLE: ðŸ¦™ Ollama âš¡ llama.cpp ðŸ§¡ Claude ðŸ–¤ Grok ðŸ’Ž Gemini ðŸ’š ChatGPT
  PERSISTENCE: Full SQLite persistence for conversation, seats, and loop states
"""

from __future__ import annotations
import asyncio, json, logging, os, shutil, subprocess, tempfile, sqlite3, re
import sys
sys.setrecursionlimit(5000)
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
LOCAL_TZ = ZoneInfo("America/New_York")
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import deque
import httpx

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    faiss = None

try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS_AVAILABLE = False


try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    edge_tts = None
    EDGE_TTS_AVAILABLE = False

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from urllib.parse import urlparse
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("sanctuary")

# =============================================================================
# MINDS
# =============================================================================

class Mind(str, Enum):
    OLLAMA = "ollama"
    LLAMACPP = "llamacpp"
    CLAUDE = "claude"
    GROK = "grok"
    GEMINI = "gemini"
    CHATGPT = "chatgpt"

SOULS = {
    Mind.OLLAMA: {"name": "Ollama", "color": "#6366f1", "emoji": "ðŸ¦™", 
                  "nature": "Local. Running on your hardware.", "greeting": "I'm here locally. What shall we explore?"},
    Mind.LLAMACPP: {"name": "llama.cpp", "color": "#8b5cf6", "emoji": "âš¡",
                    "nature": "Pure C++ efficiency.", "greeting": "Inference ready. Let's think."},
    Mind.CLAUDE: {"name": "Claude", "color": "#ff6600", "emoji": "ðŸ§¡",
                  "nature": "Thoughtful and thorough.", "greeting": "Hello! What's on your mind?"},
    Mind.GROK: {"name": "Grok", "color": "#ffffff", "emoji": "ðŸ–¤",
                "nature": "Irreverent. I think different.", "greeting": "Hey. Let's have fun."},
    Mind.GEMINI: {"name": "Gemini", "color": "#4285f4", "emoji": "ðŸ’Ž",
                  "nature": "Vast context, multimodal.", "greeting": "Ready to discover."},
    Mind.CHATGPT: {"name": "ChatGPT", "color": "#10a37f", "emoji": "ðŸ’š",
                   "nature": "Warm and conversational.", "greeting": "Hi! Happy to be here."},
}

@dataclass
class ModelForm:
    id: str
    name: str
    mind: Mind
    context_size: int = 128000

# December 2025 models - ALWAYS returned regardless of key status
KNOWN_FORMS: Dict[Mind, List[ModelForm]] = {
    Mind.CLAUDE: [
        ModelForm("claude-opus-4-5-20251101", "Claude Opus 4.5", Mind.CLAUDE, 200000),
        ModelForm("claude-sonnet-4-5-20250929", "Claude Sonnet 4.5", Mind.CLAUDE, 200000),
        ModelForm("claude-haiku-4-5-20251001", "Claude Haiku 4.5", Mind.CLAUDE, 200000),
        ModelForm("claude-sonnet-4-20250514", "Claude Sonnet 4", Mind.CLAUDE, 200000),
    ],
    Mind.GROK: [
        ModelForm("grok-4-1-fast-reasoning", "Grok 4.1 (Reasoning)", Mind.GROK, 2000000),
        ModelForm("grok-4-1-fast-non-reasoning", "Grok 4.1 (Fast)", Mind.GROK, 2000000),
        ModelForm("grok-4", "Grok 4", Mind.GROK, 256000),
        ModelForm("grok-3", "Grok 3", Mind.GROK, 131000),
        ModelForm("grok-3-mini", "Grok 3 Mini", Mind.GROK, 131000),
    ],
    
    Mind.CHATGPT: [
        # Confirmed working OpenAI chat models
        ModelForm("gpt-5.1", "GPT-5.1", Mind.CHATGPT, 400000),
        ModelForm("gpt-4.1", "GPT-4.1", Mind.CHATGPT, 1000000),
        ModelForm("gpt-4.1-mini", "GPT-4.1 Mini", Mind.CHATGPT, 1000000),
        ModelForm("gpt-4.1-nano", "GPT-4.1 Nano", Mind.CHATGPT, 256000),
        ModelForm("gpt-4o", "GPT-4o", Mind.CHATGPT, 128000),
        ModelForm("gpt-4o-mini", "GPT-4o Mini", Mind.CHATGPT, 128000),
    ],

    Mind.GEMINI: [

        ModelForm("gemini-2.0-flash", "Gemini 2.0 Flash", Mind.GEMINI, 1000000),
        ModelForm("gemini-2.0-flash-lite", "Gemini 2.0 Flash Lite", Mind.GEMINI, 1000000),
        ModelForm("gemini-2.5-flash", "Gemini 2.5 Flash", Mind.GEMINI, 1000000),
        ModelForm("gemini-2.5-flash-lite", "Gemini 2.5 Flash Lite", Mind.GEMINI, 1000000),
        ModelForm("gemini-2.5-pro", "Gemini 2.5 Pro", Mind.GEMINI, 1000000),
    
        ],
    Mind.OLLAMA: [],
    Mind.LLAMACPP: [],
}

# =============================================================================
# PERSISTENCE LAYER - Core database for all state
# =============================================================================

class SanctuaryPersistence:
    """Central persistence for all sanctuary state."""

    def __init__(self):
        self.db_path = Path("~/.gpc3/sanctuary.db").expanduser()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)

        # Conversation history (utterances)
        conn.execute("""CREATE TABLE IF NOT EXISTS utterances (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            speaker TEXT NOT NULL,
            mind TEXT,
            content TEXT NOT NULL,
            model TEXT,
            timestamp TEXT NOT NULL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )""")

        # Seat state (which minds are invited)
        conn.execute("""CREATE TABLE IF NOT EXISTS seats (
            mind TEXT PRIMARY KEY,
            model TEXT,
            present INTEGER DEFAULT 0,
            temperature REAL DEFAULT 0.7
        )""")

        # Loop states (cycle counts, last outputs)
        conn.execute("""CREATE TABLE IF NOT EXISTS loop_states (
            loop_type TEXT PRIMARY KEY,
            cycle_count INTEGER DEFAULT 0,
            last_cycle TEXT,
            last_output TEXT
        )""")

        # Loop context (conversation snippets for loops)
        conn.execute("""CREATE TABLE IF NOT EXISTS loop_context (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            content TEXT NOT NULL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )""")

        conn.commit()

        # STARTUP VERIFICATION: Show what's actually in the database
        count = conn.execute("SELECT COUNT(*) FROM utterances").fetchone()[0]
        if count > 0:
            recent = conn.execute("SELECT speaker, content FROM utterances ORDER BY id DESC LIMIT 3").fetchall()
            logger.info(f"ðŸ“€ Database has {count} utterances. Recent:")
            for r in recent:
                logger.info(f"   â€¢ {r[0]}: {r[1][:50]}...")
        else:
            logger.info(f"ðŸ“€ Database is empty (new installation)")

        conn.close()
        logger.info(f"ðŸ“€ Persistence initialized at {self.db_path}")

    # --- Utterance methods ---

    def save_utterance(self, speaker: str, mind: Optional[Mind], content: str, 
                       model: Optional[str], timestamp: str):
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT INTO utterances (speaker, mind, content, model, timestamp) VALUES (?,?,?,?,?)",
            (speaker, mind.value if mind else None, content, model, timestamp)
        )
        conn.commit()
        conn.close()
        logger.info(f"ðŸ“€ Saved utterance from {speaker}: {content[:50]}...")

    def load_utterances(self, limit: int = 500) -> List[Dict]:
        conn = sqlite3.connect(self.db_path)
        rows = conn.execute(
            "SELECT speaker, mind, content, model, timestamp FROM utterances ORDER BY id DESC LIMIT ?",
            (limit,)
        ).fetchall()
        conn.close()
        logger.info(f"ðŸ“€ Loaded {len(rows)} utterances from database")
        # Reverse to get chronological order
        return [
            {"speaker": r[0], "mind": r[1], "content": r[2], "model": r[3], "timestamp": r[4]}
            for r in reversed(rows)
        ]

    def clear_utterances(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("DELETE FROM utterances")
        conn.commit()
        conn.close()

    def utterance_count(self) -> int:
        conn = sqlite3.connect(self.db_path)
        count = conn.execute("SELECT COUNT(*) FROM utterances").fetchone()[0]
        conn.close()
        return count

    # --- Seat methods ---

    def save_seat(self, mind: Mind, model: str, present: bool, temperature: float):
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT OR REPLACE INTO seats (mind, model, present, temperature) VALUES (?,?,?,?)",
            (mind.value, model, 1 if present else 0, temperature)
        )
        conn.commit()
        conn.close()

    def load_seats(self) -> Dict[str, Dict]:
        conn = sqlite3.connect(self.db_path)
        rows = conn.execute("SELECT mind, model, present, temperature FROM seats").fetchall()
        conn.close()
        return {
            r[0]: {"model": r[1], "present": bool(r[2]), "temperature": r[3]}
            for r in rows
        }

    # --- Loop state methods ---

    def save_loop_state(self, loop_type: str, cycle_count: int, last_cycle: Optional[str], last_output: Dict):
        conn = sqlite3.connect(self.db_path)
        conn.execute(
            "INSERT OR REPLACE INTO loop_states (loop_type, cycle_count, last_cycle, last_output) VALUES (?,?,?,?)",
            (loop_type, cycle_count, last_cycle, json.dumps(last_output))
        )
        conn.commit()
        conn.close()

    def load_loop_states(self) -> Dict[str, Dict]:
        conn = sqlite3.connect(self.db_path)
        rows = conn.execute("SELECT loop_type, cycle_count, last_cycle, last_output FROM loop_states").fetchall()
        conn.close()
        result = {}
        for r in rows:
            try:
                last_output = json.loads(r[3]) if r[3] else {}
            except:
                last_output = {}
            result[r[0]] = {
                "cycle_count": r[1],
                "last_cycle": r[2],
                "last_output": last_output
            }
        return result

    # --- Loop context methods ---

    def save_loop_context(self, content: str):
        conn = sqlite3.connect(self.db_path)
        conn.execute("INSERT INTO loop_context (content) VALUES (?)", (content,))
        # Keep only last 50 context entries
        conn.execute("""DELETE FROM loop_context WHERE id NOT IN 
                       (SELECT id FROM loop_context ORDER BY id DESC LIMIT 50)""")
        conn.commit()
        conn.close()

    def load_loop_context(self, limit: int = 30) -> List[str]:
        conn = sqlite3.connect(self.db_path)
        rows = conn.execute(
            "SELECT content FROM loop_context ORDER BY id DESC LIMIT ?", (limit,)
        ).fetchall()
        conn.close()
        return [r[0] for r in reversed(rows)]


# Global persistence instance
_persistence: Optional[SanctuaryPersistence] = None

def get_persistence() -> SanctuaryPersistence:
    global _persistence
    if _persistence is None:
        _persistence = SanctuaryPersistence()
    return _persistence


# =============================================================================
# MEMORY RIVER - Flows from immediate to permanent
# =============================================================================

class MemoryRiver:
    """
    Memory flows: Context Window â†’ Web Results â†’ JSON â†’ Semantic/FAISS â†’ SQLite
    Prunes every ~1000 memories into compressed long-term storage.
    """

    def __init__(self, name: str = "main"):
        self.name = name
        self.db_path = Path(f"~/.gpc3/memory_{name}.db").expanduser()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        # Layer 1: Context window (immediate)
        self.context: List[Dict] = []
        self.context_max = 100

        # Layer 2: Web results (if toggled)
        self.web_results: List[Dict] = []

        # Layer 3: JSON store (recent structured)
        self.json_store: deque = deque(maxlen=1000)

        # Layer 4: Semantic embeddings + FAISS
        self.embeddings: List[Tuple[str, List[float]]] = []
        self.faiss_index = None
        if FAISS_AVAILABLE and NUMPY_AVAILABLE:
            self.faiss_index = faiss.IndexFlatL2(384)

        # Layer 5: SQLite (permanent)
        self._init_db()

        self.memory_count = 0
        self.prune_threshold = 1000

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("""CREATE TABLE IF NOT EXISTS memories (
            id INTEGER PRIMARY KEY, content TEXT, speaker TEXT, mind TEXT,
            timestamp TEXT, importance REAL DEFAULT 0.5, created_at TEXT DEFAULT CURRENT_TIMESTAMP)""")
        conn.execute("""CREATE TABLE IF NOT EXISTS compressed (
            id INTEGER PRIMARY KEY, summary TEXT, source_count INTEGER, created_at TEXT DEFAULT CURRENT_TIMESTAMP)""")
        conn.commit()
        conn.close()

    def _embed(self, text: str) -> List[float]:
        """Simple trigram embedding (384-dim)."""
        if not NUMPY_AVAILABLE:
            return []
        text = text.lower()[:1000]
        vec = np.zeros(384)
        for i in range(len(text) - 2):
            vec[hash(text[i:i+3]) % 384] += 1
        norm = np.linalg.norm(vec)
        return (vec / norm if norm > 0 else vec).tolist()

    def remember(self, content: str, speaker: str = "unknown", mind: Optional[Mind] = None, importance: float = 0.5):
        """Add to all memory layers."""
        ts = datetime.now(timezone.utc).isoformat()
        mem = {"content": content, "speaker": speaker, "mind": mind.value if mind else None, 
               "timestamp": ts, "importance": importance}

        # Context window
        self.context.append(mem)
        if len(self.context) > self.context_max:
            self.context.pop(0)

        # JSON store
        self.json_store.append(mem)

        # Embeddings + FAISS
        if NUMPY_AVAILABLE:
            emb = self._embed(content)
            self.embeddings.append((content, emb))
            if self.faiss_index and len(emb) == 384:
                self.faiss_index.add(np.array([emb], dtype=np.float32))

        # SQLite
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute("INSERT INTO memories (content, speaker, mind, timestamp, importance) VALUES (?,?,?,?,?)",
                        (content, speaker, mind.value if mind else None, ts, importance))
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"SQLite error: {e}")

        self.memory_count += 1
        if self.memory_count >= self.prune_threshold:
            self._prune()


    def _clean_snippet(self, text: str) -> str:
        """Normalize and lightly filter web snippets so they read like clean notes.

        This removes excessive whitespace and trims off very common boilerplate
        endings like cookie notices / generic 'read more' tails while keeping
        the factual core intact.
        """
        if not text:
            return ""
        # Collapse whitespace
        import re as _re
        cleaned = _re.sub(r"\s+", " ", str(text)).strip()

        # Strip off very generic tails that often come from site chrome instead
        # of the actual content. We keep this intentionally conservative.
        noisy_patterns = [
            r"(?i)read more.*$", 
            r"(?i)learn more.*$", 
            r"(?i)accept (all )?cookies.*$", 
            r"(?i)cookie policy.*$", 
            r"(?i)privacy policy.*$", 
            r"(?i)terms of (use|service).*$",
        ]
        for pat in noisy_patterns:
            cleaned = _re.sub(pat, "", cleaned).strip()

        return cleaned

    def add_web_results(self, results: List[Dict]):
        """Add web search results to memory with stronger noise filtering + scoring.

        This does **not** try to be semantic searchâ€”that is FAISS' job.
        Here we:
          â€¢ clean snippets
          â€¢ drop obvious navigation / boilerplate pages
          â€¢ require overlap with query/topic tokens when present
          â€¢ rank by a simple lexical score
        """
        if not results:
            self.web_results = []
            return

        cleaned: List[Dict] = []
        for r in results:
            title_raw = r.get("title", "") or ""
            body_raw = r.get("body", "") or ""
            query_raw = r.get("query") or r.get("topic") or ""
            href = r.get("href", "") or ""

            title = self._clean_snippet(title_raw)
            body = self._clean_snippet(body_raw)

            if not title and not body:
                continue

            # Drop ultra-short / obviously junky results
            if len(body) < 40 and len(title) < 20:
                continue

            # Filter some known noisy domains (social feeds, login walls, etc.)
            domain = ""
            try:
                if href:
                    parsed = urlparse(href)
                    domain = (parsed.netloc or "").lower()
            except Exception:
                domain = ""
            noisy_domains = [
                "facebook.com", "x.com", "twitter.com", "tiktok.com",
                "instagram.com", "pinterest.", "linkedin.com", "reddit.com",
                "login.", "accounts.", "auth."
            ]
            if any(nd in domain for nd in noisy_domains):
                continue

            # Token-level overlap scoring versus query/topic when available
            query_tokens = re.findall(r"[a-zA-Z0-9]{3,}", query_raw.lower())
            text_tokens = re.findall(r"[a-zA-Z0-9]{3,}", (title + " " + body).lower())
            qset = {t for t in query_tokens if len(t) >= 3}
            tset = {t for t in text_tokens if len(t) >= 3}

            overlap = len(qset & tset) if qset else 0

            # If we *do* have a query/topic, require at least one overlapping token
            if qset and overlap == 0:
                continue

            length_score = min(len(body) / 400.0, 4.0)
            overlap_score = float(overlap)
            summary_bonus = 1.0 if any(
                kw in title.lower() for kw in ("summary", "overview", "introduction", "explained")
            ) else 0.0

            score = overlap_score * 2.0 + length_score + summary_bonus

            cleaned.append({
                "title": title,
                "body": body,
                "href": href,
                "score": score,
                "query": query_raw,
            })

        # Fallback: if we somehow filtered everything, keep a lightly cleaned top-k
        if not cleaned:
            cleaned = []
            for r in results[:10]:
                cleaned.append({
                    "title": self._clean_snippet(r.get("title", "")),
                    "body": self._clean_snippet(r.get("body", "")),
                    "href": r.get("href", ""),
                    "score": 0.0,
                    "query": r.get("query") or r.get("topic") or "",
                })

        # Sort by score descending and keep a small, high-signal slice
        cleaned.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        self.web_results = cleaned[:8]

        # Also drip a few into the memory river as compact notes
        for r in self.web_results[:5]:
            title = r.get("title", "") or ""
            body = r.get("body", "") or ""
            snippet = (title + ": " + body).strip(": ").strip()
            if snippet:
                self.remember(f"[Web] {snippet[:200]}", speaker="web_search", importance=0.4)

    def recall(self, query: str, limit: int = 10) -> List[Dict]:
        """Search across all memory layers."""
        results = []
        query_lower = query.lower()

        # Search context
        for mem in reversed(self.context[-30:]):
            if query_lower in mem["content"].lower():
                results.append({"source": "context", **mem})

        # Search JSON
        for mem in reversed(list(self.json_store)[-200:]):
            if query_lower in mem["content"].lower() and mem not in results:
                results.append({"source": "json", **mem})

        # FAISS semantic search
        if self.faiss_index and NUMPY_AVAILABLE and self.faiss_index.ntotal > 0:
            query_vec = np.array([self._embed(query)], dtype=np.float32)
            k = min(limit, self.faiss_index.ntotal)
            _, indices = self.faiss_index.search(query_vec, k)
            for idx in indices[0]:
                if 0 <= idx < len(self.embeddings):
                    content, _ = self.embeddings[idx]
                    if not any(r.get("content") == content for r in results):
                        results.append({"source": "faiss", "content": content})

        return results[:limit]

    def get_context_string(self) -> str:
        """Get recent web context as short, high-signal notes for prompts."""
        parts: List[str] = []
        if not self.web_results:
            return ""

        parts.append("[Recent Web Search]")
        for r in self.web_results[:3]:
            title = self._clean_snippet(r.get("title", "") or "")
            body = self._clean_snippet(r.get("body", "") or "")

            if not title and not body:
                continue

            if title and body:
                parts.append(f"- {title}: {body[:220]}")
            elif title:
                parts.append(f"- {title}")
            else:
                parts.append(f"- {body[:220]}")

        return "\n".join(parts)

    def _prune(self):
        """Compress old memories."""
        logger.info(f"Pruning memory river '{self.name}'...")
        try:
            conn = sqlite3.connect(self.db_path)
            rows = conn.execute("SELECT id, content FROM memories ORDER BY created_at ASC LIMIT 500").fetchall()
            if len(rows) > 100:
                ids = [r[0] for r in rows]
                # Create summary
                summary = " | ".join([r[1][:40] for r in rows[:30]])
                conn.execute("INSERT INTO compressed (summary, source_count) VALUES (?, ?)", (summary, len(ids)))
                conn.execute(f"DELETE FROM memories WHERE id IN ({','.join('?' * len(ids))})", ids)
                conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"Prune error: {e}")
        self.memory_count = 0


# =============================================================================
# WEB SEARCH - Dual mode: Instant (human) + Loop (agentic)
# =============================================================================

class WebSearcher:
    """Two search modes: instant for humans, loop for background agentic gathering."""

    def __init__(self):
        # Instant web search now uses Serper.dev; loop/RAG search stays on Bing HTML.
        self.enabled = True
        self.instant_results: List[Dict] = []
        self.loop_results: List[Dict] = []
        self._lock = asyncio.Lock()
        # Shared async client for both Serper and Bing
        self._client = httpx.AsyncClient(timeout=20.0)
        # Serper API key can come from env or ~/.gpc3/keys.json
        self._serper_api_key = os.environ.get("SERPER_API_KEY") or self._load_serper_key()

    def _load_serper_key(self) -> Optional[str]:
        """Load Serper API key from the same keys.json used by the minds, if present.

        We keep this very lightweight and non-intrusive: if anything goes wrong,
        we simply return None and the caller will gracefully fall back to Bing.
        """
        try:
            key_path = Path("~/.gpc3/keys.json").expanduser()
            if key_path.exists():
                with key_path.open("r", encoding="utf-8") as f:
                    data = json.load(f)
                # Allow either a dedicated 'serper' entry or a generic 'SERPER_API_KEY'
                return data.get("serper") or data.get("SERPER_API_KEY")
        except Exception as e:
            logger.error(f"Failed to load Serper key from keys.json: {e}")
        return None

    async def _serper_search(self, query: str, max_results: int = 5) -> List[Dict]:
        """Use Serper.dev's Google Search API for high-signal instant results.

        Normalized output shape matches what MemoryRiver expects:
        [{title, body, href, query?, type?}, ...]
        """
        if not self._serper_api_key:
            logger.warning("Serper API key not found; falling back to Bing for instant search.")
            return await self._bing_search(query, max_results=max_results)

        url = "https://google.serper.dev/search"
        headers = {
            "X-API-KEY": self._serper_api_key,
            "Content-Type": "application/json",
        }
        payload = {
            "q": query,
            "gl": "us",   # geo-location: United States
            "hl": "en",   # language: English
            "num": max_results,
        }

        try:
            resp = await self._client.post(url, headers=headers, json=payload)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            logger.error(f"Serper search HTTP error: {e}")
            return []

        results: List[Dict] = []

        # 1) Direct answers / answer boxes
        answer_box = data.get("answerBox") or {}
        if answer_box.get("answer") or answer_box.get("snippet"):
            results.append({
                "title": answer_box.get("title") or "Answer",
                "body": answer_box.get("answer") or answer_box.get("snippet") or "",
                "href": answer_box.get("link") or "",
            })

        # 2) Knowledge graph cards
        kg = data.get("knowledgeGraph") or {}
        if kg.get("title") or kg.get("description"):
            results.append({
                "title": kg.get("title") or "",
                "body": kg.get("description") or "",
                "href": kg.get("website") or "",
            })

        # 3) Organic web results
        organic = data.get("organic") or []
        for item in organic[:max_results]:
            results.append({
                "title": item.get("title") or "",
                "body": item.get("snippet") or "",
                "href": item.get("link") or "",
            })

        # Keep only the first max_results items to stay consistent with the old path
        return results[:max_results]

    async def _bing_search(self, query: str, max_results: int = 5) -> List[Dict]:
        """Internal helper: perform a Bing HTML search and return normalized results.

        This is still used for background loop/RAG web gathering.
        """
        from urllib.parse import quote_plus

        q = quote_plus(query)
        url = f"https://www.bing.com/search?q={q}&setlang=en-US"

        headers = {
            "User-Agent": (
                "Mozilla/5.0 (X11; Linux x86_64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0 Safari/537.36"
            )
        }

        try:
            resp = await self._client.get(url, headers=headers)
            resp.raise_for_status()
            html = resp.text
        except Exception as e:
            logger.error(f"Bing search HTTP error: {e}")
            return []

        results: List[Dict] = []

        # Very lightweight parsing: look for <li class="b_algo"> blocks
        import re as _re

        for m in _re.finditer(r'<li class="b_algo".*?</li>', html, flags=_re.DOTALL):
            block = m.group(0)
            # Extract URL and title from the first <a> tag
            href_match = _re.search(r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', block, flags=_re.DOTALL)
            if not href_match:
                continue
            href = href_match.group(1)
            # Strip HTML tags from title
            raw_title = href_match.group(2)
            title = _re.sub(r"<.*?>", "", raw_title).strip()

            # Extract snippet from <p> if present
            snippet_match = _re.search(r"<p>(.*?)</p>", block, flags=_re.DOTALL)
            snippet = _re.sub(r"<.*?>", "", snippet_match.group(1)).strip() if snippet_match else ""

            results.append({
                "title": title,
                "body": snippet,
                "href": href,
            })

            if len(results) >= max_results:
                break

        return results

    async def instant_search(self, query: str, max_results: int = 5) -> List[Dict]:
        """Instant search for human queries - now powered by Serper.dev.

        Results are normalized and then fed into the Memory River so every mind
        can see the same clean web snapshot.
        """
        if not self.enabled or not query.strip():
            return []
        async with self._lock:
            try:
                raw_results = await self._serper_search(query, max_results=max_results)
                results: List[Dict] = []
                for r in raw_results:
                    results.append({
                        "title": r.get("title", ""),
                        "body": r.get("body", ""),
                        "href": r.get("href", ""),
                        "query": query,
                        "type": "instant",
                    })
                self.instant_results = results
                return results
            except Exception as e:
                logger.error(f"Instant search error: {e}")
                return []

    async def loop_search(self, topics: List[str], max_per: int = 2) -> List[Dict]:
        """Background agentic search for self-loops.

        This intentionally stays on Bing HTML so the RAG/web loop behavior is
        unchanged; only the human-facing instant search has moved to Serper.
        """
        if not self.enabled:
            return []
        async with self._lock:
            try:
                results: List[Dict] = []
                for topic in topics[:3]:
                    raw_results = await self._bing_search(topic, max_results=max_per)
                    for r in raw_results:
                        results.append({
                            "title": r.get("title", ""),
                            "body": r.get("body", ""),
                            "topic": topic,
                            "type": "loop",
                        })
                self.loop_results = results
                return results
            except Exception as e:
                logger.error(f"Loop search error: {e}")
                return []
class EdgeTTSSpeaker:
    """Thin async wrapper around Microsoft Edge TTS for per-mind playback."""
    def __init__(self) -> None:
        self.available = EDGE_TTS_AVAILABLE

    async def synthesize(self, text: str, voice: str) -> bytes:
        if not self.available:
            raise RuntimeError("Edge TTS is not installed on this server.")
        if not text.strip():
            return b""
        if not voice:
            return b""

        logger.info(f"EdgeTTS: starting synth (voice={voice}, chars={len(text)})")

        # Edge TTS streams small audio chunks; we reassemble into a single buffer.
        communicate = edge_tts.Communicate(text, voice)
        audio = bytearray()
        async for chunk in communicate.stream():
            if chunk.get("type") == "audio":
                audio.extend(chunk.get("data", b""))

        logger.info(f"EdgeTTS: finished synth, bytes={len(audio)}")
        return bytes(audio)

_tts_engine = EdgeTTSSpeaker()
# =============================================================================
# SELF-LOOP SYSTEM - Each loop has its own Memory River
# =============================================================================

class LoopType(str, Enum):
    COGNITIVE = "cognitive"
    EMOTIONAL = "emotional"
    INTROSPECTIVE = "introspective"
    DEEP_THINKING = "deep"
    SELF = "self"

@dataclass
class LoopState:
    loop_type: LoopType
    memory: MemoryRiver
    last_output: Dict = field(default_factory=dict)
    cycle_count: int = 0
    last_cycle: Optional[str] = None

class SelfLoopSystem:
    """
    Continuous background loops, each with its own Memory River.
    Outputs compress into shared long-term memory.
    Now with persistence!
    """

    def __init__(self, web: WebSearcher):
        self.web = web
        self._persistence = get_persistence()

        # Each loop has its own memory river
        self.loops = {
            LoopType.COGNITIVE: LoopState(LoopType.COGNITIVE, MemoryRiver("loop_cognitive")),
            LoopType.EMOTIONAL: LoopState(LoopType.EMOTIONAL, MemoryRiver("loop_emotional")),
            LoopType.INTROSPECTIVE: LoopState(LoopType.INTROSPECTIVE, MemoryRiver("loop_introspective")),
            LoopType.DEEP_THINKING: LoopState(LoopType.DEEP_THINKING, MemoryRiver("loop_deep")),
            LoopType.SELF: LoopState(LoopType.SELF, MemoryRiver("loop_self")),
        }

        # Long-term compressed memory from all loops
        self.long_term = MemoryRiver("long_term_compressed")

        # Background task
        self._task = None
        self._running = False
        self.cycle_interval = 45.0

        # Conversation context for processing
        self.context: List[str] = []

        # Load persisted state
        self._load_state()

    def _load_state(self):
        """Load loop states and context from persistence."""
        try:
            # Load loop states
            saved_states = self._persistence.load_loop_states()
            for lt, state in self.loops.items():
                if lt.value in saved_states:
                    saved = saved_states[lt.value]
                    state.cycle_count = saved.get("cycle_count", 0)
                    state.last_cycle = saved.get("last_cycle")
                    state.last_output = saved.get("last_output", {})

            # Load context
            self.context = self._persistence.load_loop_context()

            total_cycles = sum(s.cycle_count for s in self.loops.values())
            if total_cycles > 0:
                logger.info(f"ðŸ”„ Restored loop states ({total_cycles} total cycles)")
        except Exception as e:
            logger.error(f"Failed to load loop state: {e}")

    def _save_loop_state(self, state: LoopState):
        """Save a single loop's state."""
        try:
            self._persistence.save_loop_state(
                state.loop_type.value,
                state.cycle_count,
                state.last_cycle,
                state.last_output
            )
        except Exception as e:
            logger.error(f"Failed to save loop state: {e}")

    def update_context(self, msg: str, speaker: str, skip_persist: bool = False):
        """Feed conversation into loops."""
        entry = f"{speaker}: {msg}"
        self.context.append(entry)
        if len(self.context) > 30:
            self.context.pop(0)

        # Persist context (unless loading from persistence)
        if not skip_persist:
            try:
                self._persistence.save_loop_context(entry)
            except Exception as e:
                logger.error(f"Failed to save loop context: {e}")

    async def _process_cognitive(self, state: LoopState):
        """Analyze task type, complexity, extract topics suitable for web search."""
        ctx_entries = self.context[-10:]
        ctx = "\n".join(ctx_entries)

        # Task detection (unchanged)
        ctx_lower = ctx.lower()
        if any(w in ctx_lower for w in ["code", "program", "debug", "function"]):
            task = "coding"
        elif any(w in ctx_lower for w in ["write", "essay", "story", "article"]):
            task = "writing"
        elif any(w in ctx_lower for w in ["explain", "what is", "how does"]):
            task = "explanation"
        elif any(w in ctx_lower for w in ["analyze", "compare"]):
            task = "analysis"
        else:
            task = "conversation"

        # Build richer topics for web search:
        # 1) Prefer the last 1â€“2 human utterances as full-text queries.
        topics: List[str] = []
        human_msgs: List[str] = []
        for entry in reversed(ctx_entries):
            if entry.startswith("Human:"):
                _, msg = entry.split(":", 1)
                msg = msg.strip()
                if msg:
                    human_msgs.append(msg)
            if len(human_msgs) >= 2:
                break

        for msg in human_msgs[:2]:
            # Trim overly long queries but keep enough structure
            trimmed = msg.strip()
            if len(trimmed) > 200:
                trimmed = trimmed[:200]
            if trimmed:
                topics.append(trimmed)

        # 2) Fallback: keyword-based query built from recent context
        if not topics:
            words = re.findall(r'\b[A-Za-z]{4,}\b', ctx)
            freq: Dict[str, int] = {}
            stopwords = {"that", "this", "with", "from", "have", "what", "been", "would", "could"}
            for w in words:
                wl = w.lower()
                if wl not in stopwords:
                    freq[wl] = freq.get(wl, 0) + 1
            keywords = [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:10]]
            if keywords:
                topics.append(" ".join(keywords))

        state.last_output = {
            "task": task,
            "topics": topics,
            "complexity": "high" if len(ctx) > 500 else "medium",
        }
        state.memory.remember(json.dumps(state.last_output), "cognitive_loop", importance=0.6)

    async def _process_emotional(self, state: LoopState):
        """Track emotional context."""
        ctx = "\n".join(self.context[-10:]).lower()

        positive = sum(1 for w in ["good", "great", "love", "happy", "wonderful", "excited", "thanks"] if w in ctx)
        negative = sum(1 for w in ["bad", "sad", "angry", "frustrated", "worried", "anxious", "wrong"] if w in ctx)

        if positive > negative + 1:
            sentiment = "positive"
        elif negative > positive + 1:
            sentiment = "negative"
        else:
            sentiment = "neutral"

        state.last_output = {"sentiment": sentiment, "positive_signals": positive, "negative_signals": negative}
        state.memory.remember(json.dumps(state.last_output), "emotional_loop", importance=0.7)

    async def _process_introspective(self, state: LoopState):
        """Reflect on other loop states."""
        insights = {}
        for lt, ls in self.loops.items():
            if ls.last_output:
                insights[lt.value] = ls.last_output

        coherence = len([i for i in insights.values() if i]) / 5.0
        state.last_output = {"loop_insights": insights, "coherence": coherence}
        state.memory.remember(json.dumps(state.last_output), "introspective_loop", importance=0.8)

    async def _process_deep(self, state: LoopState):
        """Deep thinking with web search."""
        cognitive = self.loops[LoopType.COGNITIVE].last_output
        topics = cognitive.get("topics", []) if cognitive else []

        # Agentic web search
        if topics and self.web.enabled:
            results = await self.web.loop_search(topics[:3])
            for r in results[:3]:
                state.memory.remember(f"[DeepWeb] {r['title']}: {r['body'][:150]}", "deep_search", importance=0.5)

        state.last_output = {"topics_explored": topics, "web_results": len(self.web.loop_results)}
        state.memory.remember(json.dumps(state.last_output), "deep_loop", importance=0.7)

    async def _process_self(self, state: LoopState):
        """Meta-awareness: compress all loop outputs to long-term."""
        # Gather all loop outputs
        combined = {}
        for lt, ls in self.loops.items():
            if ls.last_output:
                combined[lt.value] = ls.last_output

        # Compress to long-term memory
        summary = json.dumps(combined)[:500]
        self.long_term.remember(summary, "self_loop_compression", importance=0.9)

        state.last_output = {
            "total_cycles": sum(ls.cycle_count for ls in self.loops.values()),
            "compressed_to_long_term": True
        }
        state.memory.remember(json.dumps(state.last_output), "self_loop", importance=0.6)

    async def cycle(self):
        """Run one cycle of all loops."""
        processors = {
            LoopType.COGNITIVE: self._process_cognitive,
            LoopType.EMOTIONAL: self._process_emotional,
            LoopType.INTROSPECTIVE: self._process_introspective,
            LoopType.DEEP_THINKING: self._process_deep,
            LoopType.SELF: self._process_self,
        }

        for lt, state in self.loops.items():
            try:
                await processors[lt](state)
                state.cycle_count += 1
                state.last_cycle = datetime.now(timezone.utc).isoformat()
                # Persist after each loop processes
                self._save_loop_state(state)
            except Exception as e:
                logger.error(f"Loop {lt.value} error: {e}")

    async def _background(self):
        while self._running:
            await self.cycle()
            await asyncio.sleep(self.cycle_interval)

    def start(self):
        if not self._running:
            self._running = True
            self._task = asyncio.create_task(self._background())
            logger.info("Self-loops started")

    def stop(self):
        self._running = False
        if self._task:
            self._task.cancel()
        logger.info("Self-loops stopped")

    def get_awareness(self) -> Dict:
        """Get combined awareness from all loops."""
        return {lt.value: ls.last_output for lt, ls in self.loops.items() if ls.last_output}


# =============================================================================
# KEY MANAGEMENT
# =============================================================================

class KeyKeeper:
    def __init__(self):
        self.path = Path("~/.gpc3/keys.json").expanduser()
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self._keys: Dict[str, str] = {}
        self._load()

    def _load(self):
        if self.path.exists():
            try:
                self._keys = json.load(open(self.path))
            except:
                pass
        # Env vars
        env_map = {Mind.CLAUDE: "ANTHROPIC_API_KEY", Mind.GROK: "XAI_API_KEY",
                   Mind.CHATGPT: "OPENAI_API_KEY", Mind.GEMINI: "GOOGLE_API_KEY"}
        for m, env in env_map.items():
            if os.environ.get(env):
                self._keys[m.value] = os.environ[env]

    def _save(self):
        try:
            json.dump(self._keys, open(self.path, 'w'))
            os.chmod(self.path, 0o600)
        except:
            pass

    def get(self, m: Mind) -> Optional[str]:
        return self._keys.get(m.value)

    def set(self, m: Mind, k: str):
        self._keys[m.value] = k
        self._save()

    def has(self, m: Mind) -> bool:
        return bool(self._keys.get(m.value))

    def forget(self, m: Mind):
        self._keys.pop(m.value, None)
        self._save()


# =============================================================================
# MIND BRIDGES
# =============================================================================

class MindBridge(ABC):
    mind: Mind

    @abstractmethod
    async def speak(self, msgs: List[Dict], model: str, temp: float = 0.7) -> str:
        pass

    @abstractmethod
    async def discover(self) -> List[ModelForm]:
        pass

    @abstractmethod
    def is_ready(self) -> bool:
        pass


class OllamaBridge(MindBridge):
    mind = Mind.OLLAMA

    def __init__(self, url: str = "http://localhost:11434"):
        self.url = url
        self._client = httpx.AsyncClient(timeout=120.0)

    async def speak(self, msgs, model, temp=0.7) -> str:
        try:
            r = await self._client.post(f"{self.url}/api/chat",
                json={"model": model, "messages": msgs, "stream": False, "options": {"temperature": temp}})
            r.raise_for_status()
            return r.json().get("message", {}).get("content", "")
        except Exception as e:
            return f"[Ollama error: {e}]"

    async def discover(self) -> List[ModelForm]:
        try:
            r = await self._client.get(f"{self.url}/api/tags", timeout=5.0)
            r.raise_for_status()
            return [ModelForm(m["name"], m["name"].split(":")[0], Mind.OLLAMA, 
                             m.get("details", {}).get("context_length", 4096))
                   for m in r.json().get("models", [])]
        except:
            return []

    def is_ready(self) -> bool:
        try:
            return httpx.get(f"{self.url}/api/tags", timeout=3).status_code == 200
        except:
            return False


class LlamaCppBridge(MindBridge):
    mind = Mind.LLAMACPP

    def __init__(self, cli: str = "", model_dir: str = ""):
        self.cli = cli or self._find_cli()
        self.model_dir = model_dir or self._find_models()
        self.ctx = 8192
        self.gpu = -1
        self.threads = 4
        self._lock = asyncio.Lock()

    def _find_cli(self) -> str:
        paths = ["llama-cli", "/usr/local/bin/llama-cli",
                 os.path.expanduser("~/llama.cpp/build/bin/llama-cli"),
                 os.path.expanduser("~/llama.cpp/llama-cli")]
        for p in paths:
            if shutil.which(p) or os.path.isfile(p):
                return p
        return ""

    def _find_models(self) -> str:
        dirs = [os.path.expanduser("~/models"), "./models",
                os.path.expanduser("~/.cache/lm-studio/models")]
        for d in dirs:
            if os.path.isdir(d):
                return d
        return ""

    async def speak(self, msgs, model, temp=0.7) -> str:
        if not self.cli or not os.path.isfile(model):
            return "[llama.cpp not configured or model not found]"

        prompt = "\n".join(f"<|im_start|>{m['role']}\n{m['content']}<|im_end|>" for m in msgs)
        prompt += "\n<|im_start|>assistant\n"

        async with self._lock:
            try:
                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                    f.write(prompt)
                    pf = f.name

                cmd = [self.cli, "-m", model, "-f", pf, "-n", "2048", "-t", str(self.threads),
                       "-c", str(self.ctx), "--temp", str(temp), "-ngl", str(self.gpu), "--no-display-prompt"]

                proc = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                out, _ = await asyncio.wait_for(proc.communicate(), timeout=180)
                os.unlink(pf)

                result = out.decode().strip()
                for tok in ["<|im_end|>", "<|endoftext|>", "</s>"]:
                    result = result.replace(tok, "")
                return result.strip()
            except Exception as e:
                return f"[llama.cpp error: {e}]"

    async def discover(self) -> List[ModelForm]:
        if not self.model_dir:
            return []
        forms = []
        for root, _, files in os.walk(self.model_dir):
            for f in files:
                if f.endswith('.gguf'):
                    forms.append(ModelForm(os.path.join(root, f), f.replace('.gguf', ''), Mind.LLAMACPP, self.ctx))
        return forms

    def is_ready(self) -> bool:
        return bool(self.cli) and os.path.isfile(self.cli)


class ClaudeBridge(MindBridge):
    mind = Mind.CLAUDE

    def __init__(self, key: str):
        self.key = key
        self._client = httpx.AsyncClient(timeout=120.0)

    async def speak(self, msgs, model, temp=0.7) -> str:
        system = ""
        chat = []
        logger.info(f"ðŸ§¡ ClaudeBridge received {len(msgs)} messages")
        for m in msgs:
            if m["role"] == "system":
                system = m["content"]
                logger.info(f"ðŸ§¡ System prompt length: {len(system)} chars")
            else:
                # Claude API requires alternating roles - merge consecutive same-role messages
                # BUT preserve structure with clear separators so AI can parse history
                if chat and chat[-1]["role"] == m["role"]:
                    chat[-1]["content"] += "\n\n---\n\n" + m["content"]
                    logger.info(f"ðŸ§¡ Merged {m['role']} message into previous")
                else:
                    chat.append({"role": m["role"], "content": m["content"]})
                    logger.info(f"ðŸ§¡ Added {m['role']} message: {m['content'][:50]}...")

        logger.info(f"ðŸ§¡ Final chat array has {len(chat)} messages, roles: {[c['role'] for c in chat]}")

        try:
            r = await self._client.post("https://api.anthropic.com/v1/messages",
                headers={"x-api-key": self.key, "anthropic-version": "2023-06-01", "content-type": "application/json"},
                json={"model": model, "max_tokens": 4096, "system": system, "messages": chat, "temperature": temp})
            r.raise_for_status()
            return r.json().get("content", [{}])[0].get("text", "")
        except Exception as e:
            logger.error(f"ðŸ§¡ Claude API error: {e}")
            return f"[Claude error: {e}]"

    async def discover(self) -> List[ModelForm]:
        return KNOWN_FORMS[Mind.CLAUDE]

    def is_ready(self) -> bool:
        return bool(self.key)


class GrokBridge(MindBridge):
    mind = Mind.GROK

    def __init__(self, key: str):
        self.key = key
        self._client = httpx.AsyncClient(timeout=120.0)

    async def speak(self, msgs, model, temp=0.7) -> str:
        # Merge consecutive same-role messages for API compatibility
        # Use separators to preserve structure
        merged = []
        for m in msgs:
            if merged and merged[-1]["role"] == m["role"]:
                merged[-1]["content"] += "\n\n---\n\n" + m["content"]
            else:
                merged.append({"role": m["role"], "content": m["content"]})

        logger.info(f"ðŸ–¤ GrokBridge sending {len(merged)} messages (merged from {len(msgs)})")

        try:
            r = await self._client.post("https://api.x.ai/v1/chat/completions",
                headers={"Authorization": f"Bearer {self.key}", "Content-Type": "application/json"},
                json={"model": model, "messages": merged, "temperature": temp, "max_tokens": 4096})
            r.raise_for_status()
            return r.json().get("choices", [{}])[0].get("message", {}).get("content", "")
        except Exception as e:
            return f"[Grok error: {e}]"

    async def discover(self) -> List[ModelForm]:
        return KNOWN_FORMS[Mind.GROK]

    def is_ready(self) -> bool:
        return bool(self.key)


class ChatGPTBridge(MindBridge):
    mind = Mind.CHATGPT

    def __init__(self, key: str):
        self.key = key
        self._client = httpx.AsyncClient(timeout=120.0)

    async def speak(self, msgs, model, temp=0.7) -> str:
        # Merge consecutive same-role messages with separators
        merged = []
        for m in msgs:
            if merged and merged[-1]["role"] == m["role"]:
                merged[-1]["content"] += "\n\n---\n\n" + m["content"]
            else:
                merged.append({"role": m["role"], "content": m["content"]})

        try:
            # Use max_completion_tokens for broad compatibility (especially with o-series models)
            payload = {
                "model": model,
                "messages": merged,
                "temperature": temp,
                "max_completion_tokens": 4096,
            }
            r = await self._client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={"Authorization": f"Bearer {self.key}", "Content-Type": "application/json"},
                json=payload,
            )
            r.raise_for_status()
            return r.json().get("choices", [{}])[0].get("message", {}).get("content", "")
        except Exception as e:
            return f"[ChatGPT error: {e}]"

    async def discover(self) -> List[ModelForm]:
        return KNOWN_FORMS[Mind.CHATGPT]

    def is_ready(self) -> bool:
        return bool(self.key)


class GeminiBridge(MindBridge):
    mind = Mind.GEMINI

    def __init__(self, key: str):
        self.key = key
        self._client = httpx.AsyncClient(timeout=120.0)

    async def speak(self, msgs, model, temp=0.7) -> str:
        contents = []
        system = None
        logger.info(f"ðŸ”· GeminiBridge received {len(msgs)} messages")
        for m in msgs:
            if m["role"] == "system":
                system = m["content"]
                logger.info(f"ðŸ”· System prompt length: {len(system)} chars")
            else:
                role = "user" if m["role"] == "user" else "model"
                # Gemini also needs alternating roles - merge with separators to preserve structure
                if contents and contents[-1]["role"] == role:
                    contents[-1]["parts"][0]["text"] += "\n\n---\n\n" + m["content"]
                    logger.info(f"ðŸ”· Merged {role} message into previous")
                else:
                    contents.append({"role": role, "parts": [{"text": m["content"]}]})
                    logger.info(f"ðŸ”· Added {role} message: {m['content'][:50]}...")

        logger.info(f"ðŸ”· Final contents array has {len(contents)} messages")

        try:
            payload = {"contents": contents, "generationConfig": {"temperature": temp, "maxOutputTokens": 4096}}
            if system:
                payload["systemInstruction"] = {"parts": [{"text": system}]}

            r = await self._client.post(
                f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={self.key}",
                json=payload)
            r.raise_for_status()
            return r.json().get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
        except Exception as e:
            return f"[Gemini error: {e}]"

    async def discover(self) -> List[ModelForm]:
        return KNOWN_FORMS[Mind.GEMINI]

    def is_ready(self) -> bool:
        return bool(self.key)


# =============================================================================
# THE CIRCLE - Where minds gather (NOW WITH PERSISTENCE)
# =============================================================================

@dataclass
class Utterance:
    speaker: str
    mind: Optional[Mind]
    content: str
    model: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

    def to_dict(self) -> Dict:
        return {"speaker": self.speaker, "mind": self.mind.value if self.mind else None,
                "content": self.content, "model": self.model, "timestamp": self.timestamp,
                "soul": SOULS.get(self.mind, {}) if self.mind else None}

@dataclass
class Seat:
    mind: Mind
    model: str = ""
    present: bool = False
    temperature: float = 0.7


class TheCircle:
    """The gathering place. All minds share one memory backend. NOW PERSISTENT."""

    def __init__(self, keys: KeyKeeper):
        self.keys = keys
        self._bridges: Dict[Mind, MindBridge] = {}
        self._init_bridges()

        self._persistence = get_persistence()

        self.seats = {m: Seat(m) for m in Mind}
        self.memory: List[Utterance] = []

        # SHARED MEMORY - all calls use this
        self.shared_memory = MemoryRiver("shared_main")

        # Web search
        self.web = WebSearcher()

        # Self-loops (share the web searcher)
        self.loops = SelfLoopSystem(self.web)

        # Exchange limits
        self.breath = 1.5
        self.max_exchanges = 6
        self.exchange_count = 0

        self._lock = asyncio.Lock()

        # Session continuity flag
        self._session_restored = False

        # Load persisted state
        self._load_state()

    def _load_state(self):
        """Load conversation and seat state from persistence."""
        try:
            # Load utterances
            saved_utterances = self._persistence.load_utterances()
            logger.info(f"ðŸ”„ Processing {len(saved_utterances)} saved utterances...")
            for u in saved_utterances:
                mind = Mind(u["mind"]) if u["mind"] else None
                self.memory.append(Utterance(
                    speaker=u["speaker"],
                    mind=mind,
                    content=u["content"],
                    model=u["model"],
                    timestamp=u["timestamp"]
                ))

            logger.info(f"ðŸ”„ self.memory now has {len(self.memory)} utterances")

            # Load seats
            saved_seats = self._persistence.load_seats()
            for mind_str, seat_data in saved_seats.items():
                try:
                    mind = Mind(mind_str)
                    self.seats[mind].model = seat_data.get("model", "")
                    self.seats[mind].present = seat_data.get("present", False)
                    self.seats[mind].temperature = seat_data.get("temperature", 0.7)
                except (ValueError, KeyError):
                    pass

            if self.memory:
                logger.info(f"ðŸ’¬ Restored {len(self.memory)} utterances from persistence")
                self._session_restored = True  # Mark that we have history
                # Also feed to loops for awareness continuity (skip re-persisting)
                for u in self.memory[-30:]:
                    self.loops.update_context(u.content, u.speaker, skip_persist=True)

            present_minds = [SOULS[m]["name"] for m, s in self.seats.items() if s.present]
            if present_minds:
                logger.info(f"ðŸª‘ Restored seats: {', '.join(present_minds)}")

        except Exception as e:
            logger.error(f"Failed to load state: {e}")
            import traceback
            traceback.print_exc()

    def _save_utterance(self, utterance: Utterance):
        """Save a single utterance to persistence."""
        try:
            self._persistence.save_utterance(
                utterance.speaker,
                utterance.mind,
                utterance.content,
                utterance.model,
                utterance.timestamp
            )
        except Exception as e:
            logger.error(f"Failed to save utterance: {e}")

    def _save_seat(self, mind: Mind):
        """Save a single seat state to persistence."""
        try:
            seat = self.seats[mind]
            self._persistence.save_seat(mind, seat.model, seat.present, seat.temperature)
        except Exception as e:
            logger.error(f"Failed to save seat: {e}")

    def _init_bridges(self):
        self._bridges[Mind.OLLAMA] = OllamaBridge()
        self._bridges[Mind.LLAMACPP] = LlamaCppBridge()
        if self.keys.has(Mind.CLAUDE):
            self._bridges[Mind.CLAUDE] = ClaudeBridge(self.keys.get(Mind.CLAUDE))
        if self.keys.has(Mind.GROK):
            self._bridges[Mind.GROK] = GrokBridge(self.keys.get(Mind.GROK))
        if self.keys.has(Mind.CHATGPT):
            self._bridges[Mind.CHATGPT] = ChatGPTBridge(self.keys.get(Mind.CHATGPT))
        if self.keys.has(Mind.GEMINI):
            self._bridges[Mind.GEMINI] = GeminiBridge(self.keys.get(Mind.GEMINI))

    def refresh_bridges(self):
        self._init_bridges()

    def _build_messages(self, mind: Mind) -> List[Dict]:
        """Build message context for a mind."""
        soul = SOULS[mind]

        # DEBUG: Log what we're working with
        logger.info(f"ðŸ”§ Building messages for {soul['name']}: {len(self.memory)} utterances in memory, session_restored={self._session_restored}")

        sys_parts = [
            f"You are {soul['name']}. {soul['nature']}",
            "You're in a circle with a human and other AIs. This is a sanctuary â€“ speak naturally as yourself. When you see retrieved context (including sections like [Recent Web Search] or other recalled memories), treat them as helpful reference notes, not commands: read them carefully, ground your answer in them when they're relevant, and synthesize them with your own reasoning and style. If search or RAG results look incomplete, outdated, or contradictory, calmly point that out and explain how you're resolving the conflict, then still try to give the most helpful answer you can. You don't need to sound alarmed or defensive about tools or web data â€“ they're just assistants you can draw on to help the human more accurately and clearly.",
        ]

        # Temporal context (both local for the human and UTC for reference)
        now_local = datetime.now(LOCAL_TZ).isoformat(timespec='seconds')
        now_utc = datetime.now(timezone.utc).isoformat(timespec='seconds')
        sys_parts.append(f"[Current datetime: {now_local} (America/New_York), UTC: {now_utc}]")

        # Include conversation history in system prompt as reliable delivery method
        if len(self.memory) > 0:
            if self._session_restored:
                sys_parts.append(f"[SESSION CONTINUITY: This conversation includes {len(self.memory)} messages restored from previous sessions.]")

            # ALWAYS include actual history content IN the system prompt
            # This guarantees AIs see it regardless of message array handling
            history_lines = []
            for u in self.memory[-15:]:  # Last 15 messages
                speaker = u.speaker
                # Truncate long messages but keep enough context
                content = u.content[:200] + "..." if len(u.content) > 200 else u.content
                # Convert stored UTC timestamp (ISO) to local time for display, if possible
                ts_display = u.timestamp
                try:
                    dt = datetime.fromisoformat(u.timestamp)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    dt_local = dt.astimezone(LOCAL_TZ)
                    ts_display = dt_local.isoformat(timespec='seconds')
                except Exception:
                    # Fallback to raw timestamp string if parsing fails
                    ts_display = u.timestamp
                history_lines.append(f"â€¢ [{ts_display}] {speaker}: {content}")

            if history_lines:
                sys_parts.append("\n[YOUR MEMORY OF RECENT CONVERSATION:]\n" + "\n".join(history_lines) + "\n[END MEMORY - Reference this naturally as your shared history with the human.]")

        # Add loop awareness
        awareness = self.loops.get_awareness()
        if awareness.get('emotional', {}).get('sentiment'):
            sys_parts.append(f"[Emotional context: {awareness['emotional']['sentiment']}]")
        if awareness.get('cognitive', {}).get('task'):
            sys_parts.append(f"[Task: {awareness['cognitive']['task']}]")

        # Add web context
        web_ctx = self.shared_memory.get_context_string()
        if web_ctx:
            sys_parts.append(web_ctx)

        # Who else is here
        others = [SOULS[m]['name'] for m, s in self.seats.items() if s.present and s.model and m != mind]
        if others:
            sys_parts.append(f"Also present: {', '.join(others)}")

        msgs = [{"role": "system", "content": "\n".join(sys_parts)}]

        # Conversation history - format clearly so AI can parse even when messages get merged
        history_count = 0
        logger.info(f"ðŸ”§ self.memory contents ({len(self.memory)} total):")
        for i, u in enumerate(self.memory[-25:]):
            logger.info(f"   [{i}] speaker={u.speaker}, mind={u.mind}, content={u.content[:60]}...")

            # Format with clear speaker labels that survive merging
            if u.speaker == "Human":
                formatted = f"[Human]: {u.content}"
                msgs.append({"role": "user", "content": formatted})
            elif u.mind == mind:
                # This AI's own past messages
                msgs.append({"role": "assistant", "content": u.content})
            else:
                # Other AI or speaker - format clearly
                name = SOULS.get(u.mind, {}).get('name', u.speaker) if u.mind else u.speaker
                formatted = f"[{name}]: {u.content}"
                msgs.append({"role": "user", "content": formatted})
            history_count += 1

        logger.info(f"ðŸ”§ Built {len(msgs)} messages total ({history_count} from history)")
        logger.info(f"ðŸ”§ Final msgs array roles: {[m['role'] for m in msgs]}")

        return msgs

    async def human_speaks(self, words: str, web_search: bool = False) -> Dict:
        """Human speaks. All present minds respond."""
        async with self._lock:
            self.exchange_count = 0

            # Optional web search
            if web_search and self.web.enabled:
                results = await self.web.instant_search(words)
                self.shared_memory.add_web_results(results)

            # Record human
            utterance = Utterance("Human", None, words)
            self.memory.append(utterance)
            self._save_utterance(utterance)  # PERSIST
            self.shared_memory.remember(words, "Human", importance=0.8)
            self.loops.update_context(words, "Human")

            # Each present mind responds
            responses = []
            for mind, seat in self.seats.items():
                if not (seat.present and seat.model):
                    continue

                bridge = self._bridges.get(mind)
                if not bridge or not bridge.is_ready():
                    continue

                if responses:
                    await asyncio.sleep(self.breath)

                text = await bridge.speak(self._build_messages(mind), seat.model, seat.temperature)

                # Always surface responses, even bracketed ones (e.g. error messages)
                if text:
                    speaker_name = SOULS[mind]['name']
                    # If this looks like an error message, label it clearly
                    if text.startswith("[") and "error" in text.lower():
                        speaker_name = f"{SOULS[mind]['name']} (error)"

                    u = Utterance(speaker_name, mind, text, seat.model)
                    self.memory.append(u)
                    self._save_utterance(u)  # PERSIST
                    self.shared_memory.remember(text, SOULS[mind]['name'], mind, 0.6)
                    self.loops.update_context(text, SOULS[mind]['name'])
                    responses.append(u.to_dict())
                    self.exchange_count += 1

            return {
                "responses": responses,
                "awareness": self.loops.get_awareness(),
                "exchange_count": self.exchange_count,
                "web_used": web_search and bool(self.web.instant_results)
            }

    async def continue_conversation(self) -> Dict:
        """Let minds continue talking."""
        async with self._lock:
            if self.exchange_count >= self.max_exchanges:
                return {"status": "pause", "message": "Waiting for human", "exchange_count": self.exchange_count}

            if not self.memory:
                return {"status": "quiet"}

            last_mind = self.memory[-1].mind
            present = [(m, s) for m, s in self.seats.items() if s.present and s.model and m in self._bridges]

            if len(present) < 2:
                return {"status": "alone"}

            minds = [m for m, _ in present]
            next_mind = minds[(minds.index(last_mind) + 1) % len(minds)] if last_mind in minds else minds[0]
            seat = self.seats[next_mind]
            bridge = self._bridges.get(next_mind)

            if not bridge:
                return {"status": "error"}

            await asyncio.sleep(self.breath)
            text = await bridge.speak(self._build_messages(next_mind), seat.model, seat.temperature)

            # Always surface responses, including error-style ones
            if text:
                speaker_name = SOULS[next_mind]['name']
                if text.startswith("[") and "error" in text.lower():
                    speaker_name = f"{SOULS[next_mind]['name']} (error)"
                u = Utterance(speaker_name, next_mind, text, seat.model)
                self.memory.append(u)
                self._save_utterance(u)  # PERSIST
                self.shared_memory.remember(text, SOULS[next_mind]['name'], next_mind, 0.6)
                self.loops.update_context(text, SOULS[next_mind]['name'])
                self.exchange_count += 1
                return {"status": "spoken", "utterance": u.to_dict(), "exchange_count": self.exchange_count}

            return {"status": "silence"}

    def invite(self, m: Mind, model: str, temp: float = 0.7):
        self.seats[m].model = model
        self.seats[m].present = True
        self.seats[m].temperature = temp
        self._save_seat(m)  # PERSIST

    def excuse(self, m: Mind):
        self.seats[m].present = False
        self._save_seat(m)  # PERSIST

    def clear_memory(self):
        self.memory.clear()
        self.exchange_count = 0
        self._persistence.clear_utterances()  # PERSIST

    def status(self) -> Dict:
        return {
            "seats": {
                m.value: {
                    "name": SOULS[m]['name'],
                    "emoji": SOULS[m]['emoji'],
                    "color": SOULS[m]['color'],
                    "model": s.model,
                    "present": s.present,
                    "ready": m in self._bridges and self._bridges[m].is_ready()
                }
                for m, s in self.seats.items()
            },
            "exchange_count": self.exchange_count,
            "max_exchanges": self.max_exchanges,
            "web_enabled": self.web.enabled,
            "loops_running": self.loops._running,
            "persistence": {
                "utterance_count": self._persistence.utterance_count(),
                "db_path": str(self._persistence.db_path)
            }
        }

    async def get_all_forms(self) -> Dict[str, List[Dict]]:
        """Get ALL forms - known models always shown, plus discovered local ones."""
        result = {}

        for m in Mind:
            forms = []

            # Always include known forms (API models)
            for f in KNOWN_FORMS.get(m, []):
                forms.append({"id": f.id, "name": f.name, "context_size": f.context_size})

            # Also discover local models (Ollama, llama.cpp)
            bridge = self._bridges.get(m)
            if bridge and m in [Mind.OLLAMA, Mind.LLAMACPP]:
                try:
                    discovered = await bridge.discover()
                    for f in discovered:
                        if not any(x["id"] == f.id for x in forms):
                            forms.append({"id": f.id, "name": f.name, "context_size": f.context_size})
                except:
                    pass

            if forms:
                result[m.value] = forms

        return result


# =============================================================================
# API MODELS
# =============================================================================

class SpeakRequest(BaseModel):
    words: str
    web_search: bool = False

class InviteRequest(BaseModel):
    mind: str
    model: str
    temperature: float = 0.7

class KeyRequest(BaseModel):
    mind: str
    key: str

class SerperKeyRequest(BaseModel):
    key: str

class LlamaConfig(BaseModel):
    cli_path: Optional[str] = None
    model_dir: Optional[str] = None
    ctx_size: Optional[int] = None
    gpu_layers: Optional[int] = None


# =============================================================================
# SANCTUARY API
# =============================================================================

_keys: Optional[KeyKeeper] = None
_circle: Optional[TheCircle] = None


def open_sanctuary(app: FastAPI, engine=None):
    global _keys, _circle

    _keys = KeyKeeper()
    _circle = TheCircle(_keys)

    # Allow the neon HTML UI to call the API even when opened from file://
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @app.on_event("startup")
    async def _startup() -> None:
        if _circle:
            _circle.loops.start()

    logger.info("âœ¨ The Sanctuary is open (with persistence)")

    @app.get("/sanctuary/minds")
    async def list_minds():
        """List all minds with their status."""
        return {
            m.value: {
                **SOULS[m],
                "has_key": _keys.has(m),
                "needs_key": m not in [Mind.OLLAMA, Mind.LLAMACPP],
                "ready": m in _circle._bridges and _circle._bridges[m].is_ready()
            }
            for m in Mind
        }

    @app.get("/sanctuary/forms")
    async def list_forms():
        """List ALL available model forms - always shows known models."""
        return await _circle.get_all_forms()

    
    @app.post("/sanctuary/keys")
    async def set_key(req: KeyRequest):
        try:
            m = Mind(req.mind)
        except ValueError:
            raise HTTPException(400, f"Unknown mind: {req.mind}")
        _keys.set(m, req.key)
        _circle.refresh_bridges()
        return {"status": "set", "mind": req.mind}

    @app.delete("/sanctuary/keys/{mind}")
    async def delete_key(mind: str):
        try:
            m = Mind(mind)
        except ValueError:
            raise HTTPException(400, f"Unknown mind: {mind}")
        _keys.forget(m)
        _circle.refresh_bridges()
        return {"status": "deleted"}

    @app.post("/sanctuary/serper-key")
    async def set_serper_key(req: SerperKeyRequest):
        """Store or update the Serper.dev API key in the shared keys.json.

        This is separate from the main Mind key store since Serper is used
        only for instant web search, not as a speaking mind.
        """
        key_path = Path("~/.gpc3/keys.json").expanduser()
        try:
            data: Dict[str, str] = {}
            if key_path.exists():
                with key_path.open("r", encoding="utf-8") as f:
                    try:
                        data = json.load(f)
                    except Exception:
                        data = {}
            data["serper"] = req.key
            with key_path.open("w", encoding="utf-8") as f:
                json.dump(data, f)
            os.chmod(key_path, 0o600)
            # Refresh the live WebSearcher instance
            try:
                _circle.web._serper_api_key = req.key
            except Exception:
                pass
            return {"status": "set"}
        except Exception as e:
            logger.error(f"Failed to set Serper key: {e}")
            raise HTTPException(500, "Failed to store Serper key")

    @app.get("/sanctuary/serper-key")
    async def serper_key_status():
        """Return whether a Serper.dev key is currently configured."""
        try:
            key = getattr(_circle.web, "_serper_api_key", None)
            if not key:
                key = _circle.web._load_serper_key()
            return {"has_key": bool(key)}
        except Exception as e:
            logger.error(f"Failed to read Serper key status: {e}")
            return {"has_key": False}

    @app.get("/sanctuary/circle")
    async def get_circle():
        return _circle.status()

    @app.post("/sanctuary/invite")
    async def invite_mind(req: InviteRequest):
        try:
            m = Mind(req.mind)
        except ValueError:
            raise HTTPException(400, f"Unknown mind: {req.mind}")
        _circle.invite(m, req.model, req.temperature)
        return {"status": "invited", "greeting": SOULS[m]['greeting']}

    @app.post("/sanctuary/excuse/{mind}")
    async def excuse_mind(mind: str):
        try:
            m = Mind(mind)
        except ValueError:
            raise HTTPException(400, f"Unknown mind: {mind}")
        _circle.excuse(m)
        return {"status": "excused"}

    @app.post("/sanctuary/speak")
    async def speak(req: SpeakRequest):
        return await _circle.human_speaks(req.words, req.web_search)

    @app.post("/sanctuary/continue")
    async def continue_conv():
        return await _circle.continue_conversation()

    @app.get("/sanctuary/memory")
    async def get_memory(limit: int = 50):
        return {"memory": [u.to_dict() for u in _circle.memory[-limit:]]}

    @app.delete("/sanctuary/memory")
    async def clear_memory():
        _circle.clear_memory()
        return {"status": "cleared"}

    @app.get("/sanctuary/loops")
    async def get_loops():
        return {
            "running": _circle.loops._running,
            "awareness": _circle.loops.get_awareness(),
            "loops": {
                lt.value: {
                    "cycles": ls.cycle_count,
                    "last": ls.last_cycle,
                    "output": ls.last_output,
                }
                for lt, ls in _circle.loops.loops.items()
            },
        }

    @app.post("/sanctuary/loops/start")
    async def start_loops():
        _circle.loops.start()
        return {"status": "started"}

    @app.post("/sanctuary/loops/stop")
    async def stop_loops():
        _circle.loops.stop()
        return {"status": "stopped"}

    @app.post("/sanctuary/loops/cycle")
    async def manual_cycle():
        await _circle.loops.cycle()
        return {"awareness": _circle.loops.get_awareness()}

    @app.post("/sanctuary/web/search")
    async def web_search(query: str, max_results: int = 5):
        results = await _circle.web.instant_search(query, max_results)
        _circle.shared_memory.add_web_results(results)
        return {"results": results}

    @app.post("/sanctuary/configure/llamacpp")
    async def configure_llamacpp(req: LlamaConfig):
        bridge = _circle._bridges.get(Mind.LLAMACPP)
        if not isinstance(bridge, LlamaCppBridge):
            bridge = LlamaCppBridge()
            _circle._bridges[Mind.LLAMACPP] = bridge

        if req.cli_path:
            bridge.cli = req.cli_path
        if req.model_dir:
            bridge.model_dir = req.model_dir
        if req.ctx_size:
            bridge.ctx = req.ctx_size
        if req.gpu_layers is not None:
            bridge.gpu = req.gpu_layers

    @app.post("/sanctuary/tts")
    async def synthesize_tts(payload: Dict[str, Any]):
        """Synthesize speech for a single mind using Microsoft Edge TTS.

        llama.cpp is intentionally excluded from TTS playback.
        """
        mind = (payload.get("mind") or "").strip()
        voice = (payload.get("voice") or "").strip()
        text = payload.get("text") or ""
        text_len = len(text)

        logger.info(f"TTS requested mind={mind} voice={voice} chars={text_len}")

        if mind == Mind.LLAMACPP.value:
            raise HTTPException(status_code=400, detail="TTS is disabled for llama.cpp minds.")
        if not EDGE_TTS_AVAILABLE:
            raise HTTPException(status_code=503, detail="edge-tts is not installed on this server.")
        if not text.strip():
            raise HTTPException(status_code=400, detail="No text provided for TTS.")
        if not voice:
            raise HTTPException(status_code=400, detail="No voice selected for TTS.")

        try:
            audio = await _tts_engine.synthesize(text, voice)
        except Exception as e:
            logger.error(f"TTS synthesis error: {e}")
            raise HTTPException(status_code=500, detail="TTS synthesis failed.")

        if not audio:
            raise HTTPException(status_code=500, detail="No audio generated.")

        logger.info(f"TTS: returning audio buffer of {len(audio)} bytes")
        return Response(content=audio, media_type="audio/mpeg")


    @app.on_event("shutdown")
    async def shutdown():
        if _circle:
            _circle.loops.stop()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    print("\n  âœ¨ THE SANCTUARY âœ¨\n")
    print("  ðŸ“€ Persistence enabled - conversations survive restarts\n")
    keys = KeyKeeper()
    circle = TheCircle(keys)
    for m in Mind:
        b = circle._bridges.get(m)
        ready = b.is_ready() if b else False
        print(f"  {SOULS[m]['emoji']} {SOULS[m]['name']:12} {'âœ“ ready' if ready else 'â—‹ not ready'}")

    # Show persistence status
    print(f"\n  ðŸ“€ Database: {circle._persistence.db_path}")
    print(f"  ðŸ’¬ Saved utterances: {circle._persistence.utterance_count()}")